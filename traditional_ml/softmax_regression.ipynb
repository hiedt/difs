{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from numba import jit\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_labels(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Convert labels to 1-hot encoding type\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): array of labels\n",
    "        C (int): total number of classes\n",
    "\n",
    "    Example:\n",
    "    y = [0,1,1,2,1]\n",
    "    C = unique(y) = 3\n",
    "    > Return Y=[[1,0,0],\n",
    "                [0,1,0],\n",
    "                [0,1,0],\n",
    "                [0,0,1],\n",
    "                [1,0,0]]\n",
    "\n",
    "    \"\"\"\n",
    "    shape = (y.size, y.max()+1)\n",
    "    output = np.zeros(shape)\n",
    "    rows = np.arange(y.size)\n",
    "    output[rows, y] = 1\n",
    "    return output.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def softmax(Z: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculate probability of each class given input Z\n",
    "\n",
    "    Args:\n",
    "        Z (np.ndarray): feature vector of size (1x2)\n",
    "        Z = W.X\n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    A = e_Z / e_Z.sum(axis=0)\n",
    "    return A"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def loss(W: np.ndarray, X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculate loss value between W.X and Y using Log-loss function\n",
    "\n",
    "    Args:\n",
    "        W (np.ndarray): Weights matrix - updated after each training iteration\n",
    "        X (np.ndarray): Feature input X\n",
    "        Y (np.ndarray): Label Y\n",
    "    \"\"\"\n",
    "    A = softmax(W.T.dot(X))\n",
    "    return -np.sum(Y*np.log(A))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grad(W, X, Y):\n",
    "    A = softmax((W.T.dot(X)))\n",
    "    E = A - Y\n",
    "    return X.dot(E.T)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def display_samples(X, label):\n",
    "    X0 = X[:, label==0]\n",
    "    X1 = X[:, label==1]\n",
    "    X2 = X[:, label==2]\n",
    "    X3 = X[:, label==3]\n",
    "    \n",
    "    plt.plot(X0[0, :], X0[1, :], 'b^', markersize = 4, alpha = .8)\n",
    "    plt.plot(X1[0, :], X1[1, :], 'go', markersize = 4, alpha = .8)\n",
    "    plt.plot(X2[0, :], X2[1, :], 'rs', markersize = 4, alpha = .8)\n",
    "    plt.plot(X3[0, :], X3[1, :], 'y*', markersize = 4, alpha = .8)\n",
    "\n",
    "    # plt.axis('off')\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    return X0, X1, X2, X3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Visualize data points & their boundaries\n",
    "xm = np.arange(-2, 11, 0.025)\n",
    "ym = np.arange(-3, 10, 0.025)\n",
    "xx, yy = np.meshgrid(xm, ym)\n",
    "\n",
    "xx1 = xx.ravel().reshape(1, xx.size)\n",
    "yy1 = yy.ravel().reshape(1, yy.size)\n",
    "\n",
    "# Make predictions on meshgrid points\n",
    "XX = np.concatenate((np.ones((1, xx.size)), xx1, yy1), axis = 0)\n",
    "\n",
    "# Create sketch frame\n",
    "fig = plt.figure()\n",
    "\n",
    "def display_boundary(weight):\n",
    "    # Make prediction with current weights\n",
    "    A = softmax(weight.T.dot(XX))\n",
    "    Z = np.argmax(A, axis=0)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, 200, cmap='jet', alpha = .1)\n",
    "    plt.xlim(-2, 11)\n",
    "    plt.ylim(-3, 10)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    display_samples(X_sample[1:, :], y_sample)\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gen_data(N: int):\n",
    "    \"\"\"\n",
    "    Generate sample dataset\n",
    "\n",
    "    Args:\n",
    "        N (int): number of samples per class\n",
    "    \n",
    "    Returns:\n",
    "        X: feature vectors of samples\n",
    "        y: label vectors of samples\n",
    "    \"\"\"\n",
    "    means = [[2, 2], [6, 2], [3, 6], [8,6]] # Centroids of clusters, each cluster is a class\n",
    "    C = len(means) # number of classes\n",
    "    cov = [[1, 0], [0, 1]] # Covariance matrix, must be a square matrix of size dxd and symmetric\n",
    "    # Generate features\n",
    "    X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "    X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "    X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "    X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "\n",
    "    X = np.concatenate((X0, X1, X2, X3), axis = 0).T # each column is a datapoint\n",
    "    X = np.concatenate((np.ones((1, C*N)), X), axis = 0)\n",
    "    # Generate labels\n",
    "    y = np.asarray([0]*N + [1]*N + [2]*N + [3]*N).T \n",
    "    \n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_sample, y_sample = gen_data(500)\n",
    "X0, X1, X2, X3 = display_samples(X_sample[1:, :], y_sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot with randomly initialized weights\n",
    "W_ini = np.random.randn(X_sample.shape[0], np.unique(y_sample).shape[0])\n",
    "# print(W_ini.shape)\n",
    "display_boundary(W_ini)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Classifier:\n",
    "    def __init__(self, eta=0.001, thresh=1e-4, steps=10_000):\n",
    "        self.W = [] # Weights, set None as initial state\n",
    "        self.eta = eta # Learning rate\n",
    "        self.thresh = thresh # Max acceptable loss for early stopping\n",
    "        self.steps = steps # Total training step\n",
    "\n",
    "\n",
    "    def train(self, X:np.ndarray, y:np.ndarray, W_init:np.ndarray=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Training function\n",
    "\n",
    "        Args:\n",
    "            W_init (np.ndarray): Initial weights of the model\n",
    "            X (np.ndarray): feature vectors\n",
    "            y (np.ndarray): true labels\n",
    "        \n",
    "        Return:\n",
    "            W (np.ndarray): final weights of the model\n",
    "        \"\"\"\n",
    "        d = X.shape[0] # Total umber of samples = C*N\n",
    "        N = X.shape[1] # Number of dimensions\n",
    "        C = np.unique(y).shape[0] # Total number of classes\n",
    "        Y = convert_labels(y) # One-hot encoded labels\n",
    "        \n",
    "        if W_init is None: # Initiate weights if not predefined\n",
    "            self.W.append(np.random.randn(d, C))\n",
    "        else:\n",
    "            self.W.append(W_init)\n",
    "            \n",
    "        step = 0\n",
    "        step_per_checkpoint = 20\n",
    "        while step < self.steps:\n",
    "            mix_id = np.random.permutation(N) # Mix data up after each epoch to ensure random selection\n",
    "            for i in mix_id:\n",
    "                # Plot process bar with tqdm\n",
    "                ## TODO\n",
    "                xi = X[:,i].reshape(d,1)\n",
    "                yi = Y[:,i].reshape(C,1)\n",
    "                ai = softmax(self.W[-1].T.dot(xi)) # Calculate prediction \n",
    "                W_new = self.W[-1] + self.eta*xi.dot((yi-ai).T) # Update new weights based on loss\n",
    "                step += 1\n",
    "                \n",
    "                # Early stopping condition in which loss is below threshold\n",
    "                if step%step_per_checkpoint == 0:\n",
    "                    if np.linalg.norm(W_new - self.W[-step_per_checkpoint]) < self.thresh:\n",
    "                        return self.W[-1]\n",
    "                self.W.append(W_new)\n",
    "\n",
    "        return self.W[-1]\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        A = softmax(self.W[-1].T.dot(X))\n",
    "        return np.argmax(A, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Classifier(steps=10_000_000) # Create classifier object\n",
    "model.train(X_sample, y_sample, W_ini) # Start training model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_boundary(model.W[-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i): \n",
    "\tax.clear()\n",
    "\n",
    "\tweight = model.W[i]\n",
    "\tA = softmax(weight.T.dot(XX))\n",
    "\tZ = np.argmax(A, axis=0)\n",
    "\tZ = Z.reshape(xx.shape)\n",
    "\n",
    "\tax.plot(X0[0, :], X0[1, :], 'b^', markersize = 4, alpha = .8)\n",
    "\tax.plot(X1[0, :], X1[1, :], 'go', markersize = 4, alpha = .8)\n",
    "\tax.plot(X2[0, :], X2[1, :], 'rs', markersize = 4, alpha = .8)\n",
    "\tax.plot(X3[0, :], X3[1, :], 'y*', markersize = 4, alpha = .8)\n",
    "\n",
    "\tax.contourf(xx, yy, Z, 200, cmap='jet', alpha = .1)\n",
    "\tax.set_title(f\"Frame number {i}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting a title for the plot \n",
    "plt.title('Creating a growing coil with matplotlib!') \n",
    "# hiding the axis details \n",
    "plt.axis('off') \n",
    "\n",
    "# call the animator\t \n",
    "anim = animation.FuncAnimation(fig, func=animate, blit=False,   \n",
    "                                interval=10, repeat=True, save_count=len(model.W))\n",
    "\n",
    "# save the animation as mp4 video file \n",
    "anim.save('model.mp4', fps=60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(model.W)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "a = np.arange(0,101)\n",
    "a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "conds = (a%10==0)\n",
    "conds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False,  True])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ee7caf1cc2aa7835c474ef31f1160da227e9c5ff1dbbd89645e8f216d435af2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('samala': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}